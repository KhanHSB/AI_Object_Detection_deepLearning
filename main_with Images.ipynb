{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AI Object Detection Using CNN with YOLO and COCO classes</h1>\n",
    "<h3>If you'd like to use this code for a video or images please scroll down to the end of the Notebook take a look at the last two cells, follow the instructions</h3>\n",
    "\n",
    "<h4>Please note that the network's been trained for use with FLIR and well as color spectrum photography.</h4>\n",
    "\n",
    "<h4>* This version is not intended for commercial use and for demonstration purposes only. </h4>\n",
    "\n",
    "<h4>* There is a much more refined version available for licensing that i've built, intended for all commercial needs. Please email me at directly at   |  haseebk73@gmail.com  |</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You're Welcome to reuse this code with proper citations.\n",
    "#Make sure you have all the dependencies installed.\n",
    "#Import Packages\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/gbusr/YAD2K provides the base work for implementation of YOLO v2 in Keras and Tensorflow\n",
    "yad2k.py converts the YOLO's Darknet weights to Keras readable format\n",
    "\"\"\"\n",
    "from yad2k.models.keras_yolo import yolo_eval, yolo_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0, 0, 255)\n",
    "model_path = 'model_data/yolo.h5'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "classes_path = 'model_data/coco_classes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection Class\n",
    "class Detector:\n",
    "    # Class initialization\n",
    "    def __init__(self, model_path, classes_path, anchors_path):\n",
    "        # Open Tensorflow session\n",
    "        self.session = K.get_session()\n",
    "        \n",
    "        # Load classes\n",
    "        with open(classes_path) as f:\n",
    "            self.class_names = f.readlines()\n",
    "            self.class_names = [c.strip() for c in self.class_names]\n",
    "            \n",
    "        # Load anchors\n",
    "        with open(anchors_path) as f:\n",
    "            self.anchors = f.readline()\n",
    "            self.anchors = [float(x) for x in self.anchors.split(',')]\n",
    "            self.anchors = np.array(self.anchors).reshape(-1, 2)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        # Check if model is fully convolutional, assuming channel last order\n",
    "        self.model_image_size = self.model.layers[0].input_shape[1:3]\n",
    "        \n",
    "        # Generate output tensor targets\n",
    "        self.input_image_shape = K.placeholder(shape = (2, ))\n",
    "        self._boxes, self._scores, self._classes = self.generate_tensor_targets(0.2, 0.4)\n",
    "        \n",
    "    # Generate output tensor targets for filtered bounding boxes\n",
    "    def generate_tensor_targets(self, score_threshold = 0.2, iou_threshold = 0.4):\n",
    "        model_outputs = yolo_head(self.model.output, self.anchors, len(self.class_names))      \n",
    "        return yolo_eval(model_outputs,\n",
    "                         self.input_image_shape,\n",
    "                         score_threshold = score_threshold,\n",
    "                         iou_threshold = iou_threshold)        \n",
    "            \n",
    "    # Resize image\n",
    "    def resize_image(self, image):\n",
    "        # Resize image\n",
    "        if self.model_image_size != (None, None):\n",
    "            # Fit to the fixed input shape\n",
    "            new_image_size = tuple(reversed(self.model_image_size))\n",
    "        else:\n",
    "            # width and height as multiples of 32.\n",
    "            new_image_size = (image.width - (image.width % 32), image.height - (image.height % 32))\n",
    "            \n",
    "        return cv2.resize(image, new_image_size)\n",
    "    \n",
    "    # Preprocess image\n",
    "    def process_image(self, image):\n",
    "        # Resize image\n",
    "        resized_image = self.resize_image(image)\n",
    "        # Convert to image to array\n",
    "        image_data = resized_image.astype(np.float32)\n",
    "        # Normalize image data\n",
    "        image_data /= 255.\n",
    "        # Add batch dimension\n",
    "        return np.expand_dims(image_data, 0), image.shape\n",
    "    \n",
    "    # Filter classes\n",
    "    def filter_classes(self, boxes, scores, classes, class_filter):\n",
    "        # If no filter\n",
    "        if len(class_filter) is 0:\n",
    "            return boxes, scores, classes\n",
    "        \n",
    "        # Filter the classes\n",
    "        out_boxes = []; out_scores = []; out_classes = []\n",
    "        for i, c in list(enumerate(classes)):\n",
    "            if self.class_names[c] in class_filter:\n",
    "                out_boxes.append(boxes[i])\n",
    "                out_scores.append(scores[i])\n",
    "                out_classes.append(classes[i])\n",
    "                \n",
    "        return out_boxes, out_scores, out_classes\n",
    "    \n",
    "    # Feed image\n",
    "    def feed(self, image):\n",
    "        image_data, image_shape = self.process_image(image)\n",
    "        # Feed image data into tensorflow\n",
    "        feed = {self.model.input: image_data,\n",
    "                self.input_image_shape: image_shape[0:2],\n",
    "                K.learning_phase(): 0}\n",
    "        # Run tensor\n",
    "        return self.session.run([self._boxes, self._scores, self._classes],\n",
    "                                                  feed_dict = feed)        \n",
    "            \n",
    "    # Feed image for detection\n",
    "    def feed_image(self, filename, class_filter):\n",
    "        # Process image for feed\n",
    "        image = cv2.imread(filename)\n",
    "        boxes, scores, classes = self.feed(image)\n",
    "        return self.filter_classes(boxes, scores, classes, class_filter)\n",
    "            \n",
    "    # Annotate the classes\n",
    "    def annotate_image(self, image, boxes, scores, classes):\n",
    "        for i, box in list(enumerate(boxes)):\n",
    "            class_name = self.class_names[classes[i]]\n",
    "            # Select annotation color\n",
    "            if class_name == 'car':\n",
    "                color = (0, 0, 255)\n",
    "            elif class_name == 'person':\n",
    "                color = (0,255, 0)\n",
    "            else:\n",
    "                color = (0, 255, 255)\n",
    "            \n",
    "            top, left, bottom, right = box\n",
    "            cv2.rectangle(image, (left, top), (right, bottom), color, 2)\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "            annotation = class_name + ': ' + str(scores[i])\n",
    "            cv2.putText(image, annotation, (left, int(top) - 5), font, 1, color, 1)\n",
    "        return image\n",
    "    \n",
    "    # Display annotated image\n",
    "    def display_image(self, image):\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        fig_size = (image.shape[1] * 16/1200)\n",
    "        fig_size = min(fig_size, 16)\n",
    "        fig_size = max(fig_size, 8)\n",
    "        plt.figure(figsize = (fig_size, fig_size))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.show()  \n",
    "        \n",
    "    # Detect and Annotate\n",
    "    def run_cars(self, filename):\n",
    "        # Feed image\n",
    "        boxes, scores, classes = self.feed_image(filename, class_filter = ['car'])\n",
    "        # Annotate image\n",
    "        image = cv2.imread(filename)\n",
    "        image = self.annotate_image(image, boxes, scores, classes)  \n",
    "        self.display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detector object\n",
    "detector = Detector(model_path, classes_path, anchors_path)\n",
    "# detector.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3718: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4172964bd5e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'images/flir9.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Annotate image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5c4dc0b52be1>\u001b[0m in \u001b[0;36mfeed_image\u001b[0;34m(self, filename, class_filter)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Process image for feed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5c4dc0b52be1>\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Feed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Feed image data into tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         feed = {self.model.input: image_data,\n",
      "\u001b[0;32m<ipython-input-8-5c4dc0b52be1>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Resize image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Convert to image to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5c4dc0b52be1>\u001b[0m in \u001b[0;36mresize_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mnew_image_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_image_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Preprocess image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3718: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "# Feed image\n",
    "filename = 'images/flir9.jpg'\n",
    "image = cv2.imread(filename)\n",
    "boxes, scores, classes = detector.feed_image(filename, class_filter = [])\n",
    "# Annotate image\n",
    "image = detector.annotate_image(image, boxes, scores, classes)\n",
    "# Display image\n",
    "detector.display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation This function is inspired by \n",
    "# https://github.com/ajayaraman/CarND-VehicleDetection/blob/master/det_vehicles.py\n",
    "def feed_video(file, class_filter, playback = False):\n",
    "    # Capture video, extract the width and height\n",
    "    video = cv2.VideoCapture(file)\n",
    "    width = int(video.get(3))\n",
    "    height = int(video.get(4))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    output_filename = os.path.join(\"output\", \"out_\" + os.path.basename(file))\n",
    "    output = cv2.VideoWriter(output_filename, fourcc, 30.0, (width, height))\n",
    "    \n",
    "    #Count to capture number of frames one is going through\n",
    "    count = 1\n",
    "    \n",
    "    #Length of video in number of frames\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(length)\n",
    "    while(count < length-50):\n",
    "        print('frame ' + str(count))\n",
    "        count += 1\n",
    "        ret, frame = video.read()\n",
    "        frameInp = cv2.COLOR_BGR2RGB\n",
    "        frame = cv2.cvtColor(frame, frameInp)\n",
    "        if ret == True:\n",
    "            boxes, scores, classes = detector.feed(frame)\n",
    "            boxes, scores, classes = detector.filter_classes(boxes, scores, classes, class_filter)\n",
    "            \n",
    "            outframe = detector.annotate_image(frame, boxes, scores, classes)        \n",
    "            outframe = cv2.cvtColor(outframe, cv2.COLOR_RGB2BGR)\n",
    "            output.write(outframe)\n",
    "\n",
    "            if playback:\n",
    "                cv2.imshow('frame', outframe)\n",
    "\n",
    "                if cv2.waitKey(0.01) & 0xFF == ord('c'):\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    video.release()\n",
    "    output.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "frame 1\n",
      "frame 2\n",
      "frame 3\n",
      "frame 4\n",
      "frame 5\n",
      "frame 6\n",
      "frame 7\n",
      "frame 8\n",
      "frame 9\n",
      "frame 10\n",
      "frame 11\n",
      "frame 12\n",
      "frame 13\n",
      "frame 14\n",
      "frame 15\n",
      "frame 16\n",
      "frame 17\n",
      "frame 18\n",
      "frame 19\n",
      "frame 20\n",
      "frame 21\n",
      "frame 22\n",
      "frame 23\n",
      "frame 24\n",
      "frame 25\n",
      "frame 26\n",
      "frame 27\n",
      "frame 28\n",
      "frame 29\n",
      "frame 30\n",
      "frame 31\n",
      "frame 32\n",
      "frame 33\n",
      "frame 34\n",
      "frame 35\n",
      "frame 36\n",
      "frame 37\n",
      "frame 38\n",
      "frame 39\n",
      "frame 40\n",
      "frame 41\n",
      "frame 42\n",
      "frame 43\n",
      "frame 44\n",
      "frame 45\n",
      "frame 46\n",
      "frame 47\n",
      "frame 48\n",
      "frame 49\n",
      "frame 50\n",
      "frame 51\n",
      "frame 52\n",
      "frame 53\n",
      "frame 54\n",
      "frame 55\n",
      "frame 56\n",
      "frame 57\n",
      "frame 58\n",
      "frame 59\n",
      "frame 60\n",
      "frame 61\n",
      "frame 62\n",
      "frame 63\n",
      "frame 64\n",
      "frame 65\n",
      "frame 66\n",
      "frame 67\n",
      "frame 68\n",
      "frame 69\n",
      "frame 70\n",
      "frame 71\n",
      "frame 72\n",
      "frame 73\n",
      "frame 74\n",
      "frame 75\n",
      "frame 76\n",
      "frame 77\n",
      "frame 78\n",
      "frame 79\n",
      "frame 80\n",
      "frame 81\n",
      "frame 82\n",
      "frame 83\n",
      "frame 84\n",
      "frame 85\n",
      "frame 86\n",
      "frame 87\n",
      "frame 88\n",
      "frame 89\n",
      "frame 90\n",
      "frame 91\n",
      "frame 92\n",
      "frame 93\n",
      "frame 94\n",
      "frame 95\n",
      "frame 96\n",
      "frame 97\n",
      "frame 98\n",
      "frame 99\n",
      "frame 100\n",
      "frame 101\n",
      "frame 102\n",
      "frame 103\n",
      "frame 104\n",
      "frame 105\n",
      "frame 106\n",
      "frame 107\n",
      "frame 108\n",
      "frame 109\n",
      "frame 110\n",
      "frame 111\n",
      "frame 112\n",
      "frame 113\n",
      "frame 114\n",
      "frame 115\n",
      "frame 116\n",
      "frame 117\n",
      "frame 118\n",
      "frame 119\n",
      "frame 120\n",
      "frame 121\n",
      "frame 122\n",
      "frame 123\n",
      "frame 124\n",
      "frame 125\n",
      "frame 126\n",
      "frame 127\n",
      "frame 128\n",
      "frame 129\n",
      "frame 130\n",
      "frame 131\n",
      "frame 132\n",
      "frame 133\n",
      "frame 134\n",
      "frame 135\n",
      "frame 136\n",
      "frame 137\n",
      "frame 138\n",
      "frame 139\n",
      "frame 140\n",
      "frame 141\n",
      "frame 142\n",
      "frame 143\n",
      "frame 144\n",
      "frame 145\n",
      "frame 146\n",
      "frame 147\n",
      "frame 148\n",
      "frame 149\n",
      "frame 150\n",
      "frame 151\n",
      "frame 152\n",
      "frame 153\n",
      "frame 154\n",
      "frame 155\n",
      "frame 156\n",
      "frame 157\n",
      "frame 158\n",
      "frame 159\n",
      "frame 160\n",
      "frame 161\n",
      "frame 162\n",
      "frame 163\n",
      "frame 164\n",
      "frame 165\n",
      "frame 166\n",
      "frame 167\n",
      "frame 168\n",
      "frame 169\n",
      "frame 170\n",
      "frame 171\n",
      "frame 172\n",
      "frame 173\n",
      "frame 174\n",
      "frame 175\n",
      "frame 176\n",
      "frame 177\n",
      "frame 178\n",
      "frame 179\n",
      "frame 180\n",
      "frame 181\n",
      "frame 182\n",
      "frame 183\n",
      "frame 184\n",
      "frame 185\n",
      "frame 186\n",
      "frame 187\n",
      "frame 188\n",
      "frame 189\n",
      "frame 190\n",
      "frame 191\n",
      "frame 192\n",
      "frame 193\n",
      "frame 194\n",
      "frame 195\n",
      "frame 196\n",
      "frame 197\n",
      "frame 198\n",
      "frame 199\n",
      "frame 200\n",
      "frame 201\n",
      "frame 202\n",
      "frame 203\n",
      "frame 204\n",
      "frame 205\n",
      "frame 206\n",
      "frame 207\n",
      "frame 208\n",
      "frame 209\n",
      "frame 210\n",
      "frame 211\n",
      "frame 212\n",
      "frame 213\n",
      "frame 214\n",
      "frame 215\n",
      "frame 216\n",
      "frame 217\n",
      "frame 218\n",
      "frame 219\n",
      "frame 220\n",
      "frame 221\n",
      "frame 222\n",
      "frame 223\n",
      "frame 224\n",
      "frame 225\n",
      "frame 226\n",
      "frame 227\n",
      "frame 228\n",
      "frame 229\n",
      "frame 230\n",
      "frame 231\n",
      "frame 232\n",
      "frame 233\n",
      "frame 234\n",
      "frame 235\n",
      "frame 236\n",
      "frame 237\n",
      "frame 238\n",
      "frame 239\n",
      "frame 240\n",
      "frame 241\n",
      "frame 242\n",
      "frame 243\n",
      "frame 244\n",
      "frame 245\n",
      "frame 246\n",
      "frame 247\n"
     ]
    }
   ],
   "source": [
    "#Place your video file in the images folder and enter the filename here.\n",
    "#Will return your output file to the output folder.\n",
    "feed_video('images/flv1.mp4', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
